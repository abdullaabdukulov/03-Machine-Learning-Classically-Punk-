{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install librosa\n",
    "%pip install tensorflow\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import callbacks as cb\n",
    "import os, json, math, librosa\n",
    "import IPython.display as ipd\n",
    "import librosa.display as dis\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Dropout, Flatten, MaxPooling2D, Input, Activation\n",
    "\n",
    "import sklearn.model_selection as sk\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"country.mp3\"\n",
    "print(file)\n",
    "\n",
    "\n",
    "# waveform\n",
    "signal, sr = librosa.load(file, sr=22050)\n",
    "plt.plot(signal)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Amp\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# fft -> spectrum\n",
    "fft = np.fft.fft(signal)\n",
    "\n",
    "magnitude = np.abs(fft)\n",
    "frequency = np.linspace(0, sr, len(magnitude))\n",
    "left_freq = frequency[: int(len(frequency) / 2)]\n",
    "left_mag = magnitude[: int(len(magnitude) / 2)]\n",
    "\n",
    "plt.plot(left_freq, left_mag, color=\"b\")\n",
    "plt.xlabel(\"freq\")\n",
    "plt.ylabel(\"mag\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# fft\n",
    "n_fft = 2048  # window when considering performing a single fft\n",
    "hop_length = 512  # amount shifting after each transform\n",
    "\n",
    "stft = librosa.core.stft(signal, hop_length=hop_length, n_fft=n_fft)\n",
    "spectrogram = np.abs(stft)\n",
    "\n",
    "log_spectrogram = librosa.amplitude_to_db(spectrogram)\n",
    "\n",
    "librosa.display.specshow(log_spectrogram, sr=sr, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# mfcc\n",
    "MFCCs = librosa.feature.mfcc(y=signal, n_fft=n_fft, hop_length=hop_length, n_mfcc=13)\n",
    "librosa.display.specshow(MFCCs, sr=sr, hop_length=hop_length)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"MFCC\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Genres from folder name\n",
    "\n",
    "MUSIC = 'genres'\n",
    "music_dataset = [] # File locations for each wav file \n",
    "genre_target = [] # \n",
    "for root, dirs, files in os.walk(MUSIC):\n",
    "    for name in files:\n",
    "        filename = os.path.join(root, name)\n",
    "        if filename != 'genres/blues/blues.00054.wav':\n",
    "            music_dataset.append(filename)\n",
    "            genre_target.append(filename.split(\"/\"))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"genres in dataset: \", genre_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = music_dataset[500]\n",
    "\n",
    "x , sr = librosa.load(audio_path)\n",
    "librosa.load(audio_path, sr=None)\n",
    "\n",
    "ipd.Audio(audio_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Audio File as a waveform\n",
    "plt.figure(figsize=(16, 5))\n",
    "# librosa.display.waveplot(x, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing audio file as a spectogram\n",
    "X = librosa.stft(x)\n",
    "Xdb = librosa.amplitude_to_db(abs(X))\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
    "plt.title('Spectogram')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = audio_path\n",
    "y, sr = librosa.load(file_location)\n",
    "melSpec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "melSpec_dB = librosa.power_to_db(melSpec, ref=np.max)\n",
    "plt.figure(figsize=(10, 5))\n",
    "librosa.display.specshow(melSpec_dB, x_axis='time', y_axis='mel', sr=sr, fmax=8000)\n",
    "plt.colorbar(format='%+1.0f dB')\n",
    "plt.title(\"MelSpectrogram\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'genres'\n",
    "JSON_PATH = \"data_10.json\"\n",
    "SAMPLE_RATE = 22050\n",
    "TRACK_DURATION = 30\n",
    "SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION\n",
    "\n",
    "\n",
    "def save_mfcc(dataset_path, json_path, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):\n",
    "    data = {\n",
    "        \"mapping\": [],\n",
    "        \"labels\": [],\n",
    "        \"mfcc\": []\n",
    "    }\n",
    "\n",
    "    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
    "    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
    "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(dataset_path)):\n",
    "        if dirpath is not dataset_path:\n",
    "            semantic_label = dirpath.split(\"/\")[-1]\n",
    "            data[\"mapping\"].append(semantic_label)\n",
    "            print(\"\\nProcessing: {}\".format(semantic_label))\n",
    "            for f in filenames:\n",
    "                file_path = os.path.join(dirpath, f)\n",
    "                if file_path != 'genres/blues/blues.00054.wav':\n",
    "                    signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "                    for d in range(num_segments):\n",
    "                        start = samples_per_segment * d\n",
    "                        finish = start + samples_per_segment\n",
    "                        mfcc = librosa.feature.mfcc(y=signal[start:finish], sr=sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "                        mfcc = mfcc.T\n",
    "                        if len(mfcc) == num_mfcc_vectors_per_segment:\n",
    "                            data[\"mfcc\"].append(mfcc.tolist())\n",
    "                            data[\"labels\"].append(i-1)\n",
    "    with open(json_path, \"w\") as fp:\n",
    "        json.dump(data, fp, indent=4)\n",
    "save_mfcc(DATASET_PATH, JSON_PATH, num_segments=15)\n",
    "print(\"process finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data_10.json\"\n",
    "\n",
    "def load_data(data_path):\n",
    "\n",
    "    with open(data_path, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "\n",
    "    X = np.array(data[\"mfcc\"])\n",
    "    y = np.array(data[\"labels\"])\n",
    "    z = np.array(data['mapping'])\n",
    "    return X, y, z\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].plot(history.history[\"accuracy\"], label=\"train accuracy\")\n",
    "    axs[0].plot(history.history[\"val_accuracy\"], label=\"test accuracy\")\n",
    "    axs[0].set_ylabel(\"Accuracy\")\n",
    "    axs[0].legend(loc=\"lower right\")\n",
    "    axs[0].set_title(\"Accuracy eval\")\n",
    "    \n",
    "    axs[1].plot(history.history[\"loss\"], label=\"train error\")\n",
    "    axs[1].plot(history.history[\"val_loss\"], label=\"test error\")\n",
    "    axs[1].set_ylabel(\"Error\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].legend(loc=\"upper right\")\n",
    "    axs[1].set_title(\"Error eval\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def prepare_datasets(test_size, validation_size):\n",
    "    X, y, z = load_data(DATA_PATH)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle = True,random_state =42)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=1, shuffle = True, random_state = 42)\n",
    "\n",
    "    # add an axis to input sets\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "    X_validation = X_validation[..., np.newaxis]\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "\n",
    "    return X_train, X_validation, X_test, y_train, y_validation, y_test, z\n",
    "\n",
    "\n",
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "#     1st conv layer\n",
    "    model.add(keras.layers.Conv2D(32, (2, 2), activation='relu', input_shape=input_shape, kernel_initializer = 'he_normal'))\n",
    "    model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "    # 2nd conv layer\n",
    "    model.add(keras.layers.Conv2D(32, (2, 2), activation='relu', kernel_initializer = 'he_normal'))\n",
    "    model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "    # 3rd conv layer\n",
    "    model.add(keras.layers.Conv2D(32, (2, 2), activation='relu', kernel_initializer = 'he_normal'))\n",
    "    model.add(keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "    # flatten output and feed it into dense layer\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(128, activation='relu', kernel_initializer = 'he_normal'))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', kernel_initializer = 'he_normal'))\n",
    "    model.add(keras.layers.Dropout(0.5))\n",
    "    # output layer\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(model, X, y):\n",
    "    \n",
    "    X = X[np.newaxis, ...]\n",
    "    prediction = model.predict(X)\n",
    "    predicted_index = np.argmax(prediction, axis=1)\n",
    "    target = z[y]\n",
    "    predicted = z[predicted_index]\n",
    "\n",
    "    print(\"Target: {}, Predicted label: {}\".format(target, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train, validation, test splits\n",
    "X_train, X_validation, X_test, y_train, y_validation, y_test, z = prepare_datasets(0.1, 0)\n",
    "\n",
    "# create network\n",
    "input_shape = (X_train.shape[1], X_train.shape[2], 1)\n",
    "model = build_model(input_shape)\n",
    "\n",
    "# compile model\n",
    "optimiser = keras.optimizers.Adam(learning_rate=0.0025)\n",
    "model.compile(optimizer=optimiser,loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "my_callbacks = [tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = \"callbacks.keras\",\n",
    "    monitor = \"val_loss\",\n",
    "    mode = \"min\"\n",
    ")]\n",
    "model.summary()\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=512, epochs=200, verbose = 2, callbacks = my_callbacks)\n",
    "# plot accuracy/error for training and validation\n",
    "plot_history(history)\n",
    "\n",
    "# evaluate model on test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a sample to predict from the test set\n",
    "X_to_predict = X_test\n",
    "y_to_predict = y_test\n",
    "# predict sample\n",
    "predict(model, X_to_predict, y_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
